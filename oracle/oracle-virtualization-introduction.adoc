---
sidebar: sidebar
permalink: oracle/oracle-virtualization-introduction.html
keywords: oracle, database, ontap, virtualization, esx, hyper-v, KVM
summary: Introduction to database virtualization
---
= Virtualization

:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
Virtualization of databases with VMware, Oracle OLVM, or KVM is an increasingly common choice for NetApp customers who chose virtualization for even their most mission-critical databases.

== Supportability

Many misconceptions exist on the support policies for virtualization, particularly for VMware products. It is not uncommon to hear that Oracle outright does not support virtualization. This notion is incorrect and leads to missed opportunities for virtualization. Oracle Doc ID 249212.1 discusses known issues in an Oracle environment and also specifies support for RAC.

A customer with a problem that occurs on a virtualized server and is previously unknown to Oracle Support might be asked to reproduce the problem on physical hardware. An Oracle customer running a bleeding-edge version of a product might not want to use virtualization because of the potential for new bug discovery. However, this situation has not been a problem in practice for virtualization customers using generally available product versions.

== Storage presentation

Customers considering virtualization of their databases should base their storage decisions on their business needs. Although this is a generally true statement for all IT decisions, it is especially important for virtualization, because the size and scope of projects vary considerably.

There are three basic options for storage presentation:

* Hypervisor datastores
* iSCSI LUNs managed by the iSCSI initiator on the VM, not the hypervisor
* NFS file systems mounted by the VM (not via an NFS-based datastore)
* Direct device mappings. VMware RDMs are disfavored, but physical devices are still often directly mapped to KVM and OLVM hypervisors.

=== Performance

The method of presenting storage to a virtualized guest does not generally affect performance. Host OSs, virtualized network drivers, and hypervisor datastore implementations are all highly optimized and can generally consume all available FC or IP network bandwidth between the hypervisor and the storage system. In some cases, obtaining optimal performance might be slightly easier using one storage presentation approach as compared to another, but the end result should be comparable. 

=== Manageability

The key factor in deciding how to present storage to a virtualized guest is mangeability. There is not right or wrong answer. The best approach depends on IT operational preferences.

* *Transparency.* When a VM manages its file systems, it is easier for a database administrator or a system administrator to identify the source of the file systems for their data. The filesystems and LUNs are accessed no differently than with a physical server. 
* *Consistency.* When a VM owns its file systems, the use or nonuse of a hypervisor layer affects manageability. The same procedures for provisioning, monitoring, data protection, and so on can be used across the entire estate, including both virtualized and nonvirtualized environments.
* *Stability and troubleshooting.* When a VM owns its file systems, delivering good, stable performance and troubleshooting problems are much simpler because the entire storage stack is present on the VM. The hypervisor's only role is to transport FC or IP frames. When a datastore is included in a configuration, it slightly complicates the configuration by introducing another set of timeouts, parameters, log files, and potential bugs. 
* *Portability.* When a VM owns its file systems, the process of moving an Oracle environment becomes much simpler. File systems can easily be moved between virtualized and nonvirtualized guests.
* *Vendor lock-in.* After data is placed in a datastore, using a different hypervisor or taking the data out of the virtualized environment entirely becomes difficult.
* *Snapshot enablement.* In some cases, backups in a virtualized environment can become a problem because of the relatively limited bandwidth. For example, a four-port 10GbE trunk might be sufficient to support the day-to-day performance needs of many virtualized databases. However, such a trunk would be insufficient to perform backups using RMAN or other backup products that require streaming a full-sized copy of the data. The result is that an increasingly consolidated virtualized environment needs snapshots. This avoids the need to overbuild the hypervisor configuration purely to support the bandwidth and CPU requirements in the backup window.
+
Using guest-owned file systems sometimes makes it easier to leverage Snapshot-based backups and restores because the storage objects in need of protection can be targeted more easily. However, there are an increasingly large number of virtualization data protection products that integrate well with datastores and snapshots. The backup strategy should be fully considred before making a decision on how to present storage to a virtualized host.

== Paravirtualized drivers
For optimum performance, the use of paravirtualized network drivers is critical. When a datastore is used, a paravirtualized SCSI driver is required. A paravirtualized device driver allows a guest to integrate more deeply into the hypervisor, as opposed to an emulated driver in which the hypervisor spends more CPU time mimicking the behavior of physical hardware.

== Overcommitting RAM
Overcommitting RAM means configuring more virtualized RAM on various hosts than exists on the physical hardware. Doing so can cause unexpected performance problems. When virtualizing a database, the underlying blocks of the Oracle SGA must not be swapped out to storage by the hypervisor. Doing so causes highly unstable performance results.

== Datastore striping

When using a datastore, there is one critical factor to consider with respect to performance - striping.

Datatore technologies such as VMFS are able to span multiple LUNs, but they are not stripe devices. They are concatenated. The end result can be LUN hot spots. For example, a typical Oracle database might have an 8-LUN ASM diskgroup. All 8 virtualized LUNs could be provisioned on an 8-LUN VMFS datastore, but there is no guarantee on which LUNs the data will reside. The resulting configuration could be all 8 virtualized LUN occupying a single LUN within the VMFS datastore.

Striping is required. With some hypervisors, including KVM, it is possible to built a datastore using LVM striping as described link:oracle-storage-san-config-lvm-striping.html[here]. With VMware, the architecture looks a little different. Each virtualized LUN needs to be placed on a different VMFS datastore. 

For example: 

image:vmfs-striping.png[Error: Missing Graphic Image]

The primary driver for this approach is not ONTAP, it's because of inherent limitations in the number of operations a single VM or hypervisor LUN can service in parallel. A single ONTAP LUN can generally support far more IOPS than a host can request. The result is most databases need between 4 and 8 LUNs supporting their workloads. 

VMware architectures would need to plan their architectures carefully to ensure that datastore and/or LUN path maximumes are not encountered with this approach. Furthermore, there is no requirement for a unique set of VMFS datastores for every database. The primary need is to ensure each host has a clean set of IO paths from the virtualized LUNs to the backend LUNs on the storage system itself. 