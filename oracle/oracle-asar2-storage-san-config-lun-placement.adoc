---
sidebar: sidebar
permalink: oracle/oracle-asar2-storage-san-config-lun-placement.html
keywords: SAN, ASM, LVM, oracle, LUN, ASA r2
summary: Oracle and ONTAP LUN placement with ASA r2
searchtitle: Oracle database LUN placement with ASA r2
---
= LUN placement
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ../media/

[.lead]
Optimal placement of database LUNs within ASA r2 systems primarily depends on how various ONTAP features will be used.

In ASA r2 systems, storage units (LUNs or NVMe namespaces) are created from a simplified storage layer called Storage Availability Zones (SAZs), which act as common pools of storage for an HA pair.

== Storage Availability Zones (SAZs)

One common point of confusion with customers new to ASA r2 is the absence of FlexVols, commonly referred to as "volumes" in traditional ONTAP systems.

ASA r2 does not use volumes as management containers. Instead, storage units (LUNs or NVMe namespaces) are provisioned directly within Storage Availability Zones (SAZs) through Storage VMs (SVMs). SAZs are simplified pools of storage for an HA pair, and ONTAP automatically creates the necessary internal structures when you provision a storage unit.

A storage unit is not a SAZ. These terms are often conflated because other vendor products use similar terminology. In ASA r2, storage units are the actual data-serving entities for hosts, while SAZs exist to simplify SAN management and abstract away aggregates. This design eliminates the need for manual volume management and makes provisioning more direct and streamlined for block workloads like Oracle databases.

== SAZs and Storage units

Related storage units (LUNs or NVMe namespaces) are normally co-located within a single Storage Availability Zone (SAZ). For example, a database that requires 10 storage units (LUNs) would typically have all 10 units placed in the same SAZ for simplicity and performance.

[CAUTION]
====
* Using a 1:1 ratio of storage units to SAZs, meaning one storage unit (LUN) per SAZ, is not a formal best practice. 
* Instead, SAZs should be viewed as containers for workloads or datasets. There may be a single storage unit per SAZ, or there could be many. The right answer depends on manageability requirements.
* Scattering storage units across an unnecessary number of SAZs can lead to additional overhead and scheduling problems for operations such as snapshot operations, excessive numbers of objects displayed in the UI, and result in reaching platform SAZ limits before the storage unit limit is reached.
====

[NOTE]
In context of FC SAN, here storage unit refers to LUN.

== Consistency Groups (CGs), LUNs, and snapshots

In ASA r2, snapshot policies and schedules are applied at the Consistency Group level, which is a logical construct that groups multiple LUNs or NVMe namespaces for coordinated data protection. A dataset that consists of 10 LUNs would require only a single snapshot policy when those LUNs are part of the same Consistency Group.

Consistency Groups ensure atomic snapshot operations across all included LUNs. For example, a database that resides on 10 LUNs, or a VMware-based application environment consisting of 10 different OSs, can be protected as a single, consistent object if the underlying LUNs are grouped in the same consistency group. If they are placed in different consistency groups, snapshots may or may not be perfectly synchronized, even if scheduled at the same time.

In some cases, a related set of LUNs might need to be split into two different consistency groups because of recovery requirements. For example, a database might have four LUNs for datafiles and two LUNs for logs. In this case, a datafile consistency group with 4 LUNs and a log consistency group with 2 LUNs might be the best option. The reason is independent recoverability: the datafile consistency group could be selectively restored to an earlier state, meaning all four LUNs would be reverted to the state of the snapshot, while the log consistency group with its critical data would remain unaffected.

== CGs, LUNs, and SnapMirror

SnapMirror policies and operations are, like snapshot operations, performed on the consistency group, not the LUN.

Co-locating related LUNs in a single consistency group allows you to create a single SnapMirror relationship and update all contained data with a single update. As with snapshots, the update will also be an atomic operation. The SnapMirror destination would be guaranteed to have a single point-in-time replica of the source LUNs. If the LUNs were spread across multiple consistency groups, the replicas may or may not be consistent with one another.

[NOTE]
====

SnapMirror replication on ASA r2 systems has the following limitations:

* SnapMirror synchronous replication is not supported. 
* SnapMirror active sync is supported only between two ASA r2 systems. 
* SnapMirror asynchronous replication is supported only between two ASA r2 systems. 
* SnapMirror asynchronous replication is not supported between an ASA r2 system and an ASA, AFF or FAS system or the cloud.

Learn more about https://docs.netapp.com/us-en/asa-r2/data-protection/pre-defined-protection-policies.html[SnapMirror replication policies supported on ASA r2 systems].
====

== CGs, LUNs, and QoS

While QoS can be selectively applied to individual LUNs, it is usually easier to set it at the consistency group level. For example, all of the LUNs used by the guests in a given ESX server could be placed in a single consistency group, and then an ONTAP adaptive QoS policy could be applied. The result is a self-scaling IOPS-per-TB limit that applies to all LUNs.

Likewise, if a database required 100K IOPS and occupied 10 LUNs, it would be easier to set a single 100K IOPS limit on a single consistency group than to set 10 individual 10K IOPS limits, one on each LUN.

== Multiple CG layouts

There are some cases where distributing LUNs across multiple consistency groups may be beneficial. The primary reason is controller striping. For example, an HA ASA r2 storage system might be hosting a single Oracle database where the full processing and caching potential of each controller is required. In this case, a typical design would be to place half of the LUNs in a single consistency group on controller 1, and the other half of the LUNs in a single consistency group on controller 2.

Similarly, for environments hosting many databases, distributing LUNs across multiple consistency groups can ensure balanced controller utilization. For example, an HA system hosting 100 databases of 10 LUNs each might assign 5 LUNs to a consistency group on controller 1 and 5 LUNs to a consistency group on controller 2 per database. This guarantees symmetric loading as additional databases are provisioned.

None of these examples involve a 1:1 LUN-to-consistency group ratio, though. The goal remains to optimize manageability by grouping related LUNs logically in consistency group.

One example where a 1:1 LUN to consistency group ratio makes sense is containerized workloads, where each LUN might really represent a single workload requiring separate snapshot and replication policies and thus need to be managed on an individual basis. In such cases, a 1:1 ratio may be optimal.