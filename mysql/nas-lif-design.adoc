---
sidebar: sidebar
permalink: mysql/nas-lif-design.html
keywords: MySQL,MariaDB,Percona,database
summary: MySQL on ONTAP
---

= NFS LIF Design

In contrast to SAN protocols, NFSv3 and NFSv4 have a limited ability to define multiple paths to data. The parallel NFS (pNFS) extensions to NFSv4.1 address this limitation. 

== Performance and Resiliency

Although measuring SAN LIF performance is primarily a matter of calculating the total bandwidth from all primary paths, determining NFS LIF performance requires a closer look at the exact network configuration. For example, two 10Gb ports can be configured as raw physical ports, or they can be configured as a Link Aggregation Control Protocol (LACP) interface group. If they are configured as an interface group, multiple load-balancing policies are available that work differently depending on whether traffic is switched or routed. Finally, Direct NFS (DNFS) offers load-balancing configurations that do not exist in any currently operating system NFS clients.

Unlike SAN protocols, NFS file systems require resiliency at the protocol layer. For example, a LUN is always configured with multipathing enabled, meaning that multiple redundant channels are available to the storage system, each of which uses the FC protocol. An NFS file system, on the other hand, depends on the availability of a single TCP/IP channel that can be protected at the physical layer only. This arrangement is why options such as port failover and LACP port aggregation exist.

In an NFS environment, both performance and resiliency are provided at the network protocol layer. As a result, both topics are intertwined and must be discussed together.

== Bind LIFs to Port Groups

To bind a LIF to a port group, associate the LIF IP address with a group of physical ports. The primary method for aggregating physical ports together is LACP. The fault-tolerance capability of LACP is fairly simple; each port in an LACP group is monitored and is removed from the port group if a malfunction occurs. There are, however, many misconceptions about how LACP works with respect to performance:

* LACP does not require the configuration on the switch to match the endpoint. For example, ONTAP can be configured with IP-based load balancing, whereas a switch can use MAC-based load balancing.
* Each endpoint using an LACP connection can independently choose the packet transmission port, but it cannot choose the port used for receipt. Therefore, traffic from ONTAP to a particular destination is tied to a particular port, and the return traffic could arrive on a different interface. This does not cause problems, however.
* LACP does not evenly distribute traffic all the time. In a large environment with many NFS clients, the result is typically even use of all ports in an LACP aggregation. However, any one NFS file system in the environment is limited to the bandwidth of only one port, not the entire aggregation.
* Although round-robin LACP policies are available on ONTAP, these policies do not address the connection from a switch to a host. For example, a configuration with a four-port LACP trunk on a host and a four-port LACP trunk on ONTAP is still only able to read a file system using a single port. Although ONTAP can transmit data through all four ports, no switch technologies are currently available that send from the switch to the host through all four ports. Only one is used.

The most common approach in larger environments consisting of many database hosts is to build an LACP aggregate of an appropriate number of 10Gb interfaces by using IP load balancing. This approach enables ONTAP to deliver even use of all ports, as long as enough clients exist. Load balancing breaks down when there are fewer clients in the configuration, because LACP trunking does not dynamically redistribute load. 

When a connection is established, traffic in a particular direction is placed on only one port. For example, a database performing a full table scan against an NFS file system connected through a four-port LACP trunk reads data though only one network interface card (NIC). If only three database servers are in such an environment, it is possible that all three are reading from the same port, while the other three ports are idle.

== Bind LIFs to Physical Ports

Binding a LIF to a physical port results in more granular control over network configuration because a given IP address on an ONTAP system is associated with only one network port at a time. Resiliency is then accomplished through the configuration of failover groups and failover policies.

== Failover Policies and Failover Groups

Data ONTAP 8.3 and higher allows management of LIF failover based on broadcast domains. Therefore, an administrator can define all the ports that have access to a given subnet and allow Data ONTAP to select an appropriate failover LIF. Some customers can use this approach, but it has limitations in a high-speed database storage network environment because of the lack of predictability. For example, an environment can include both 1Gb ports for routine file system access and 10Gb ports for data file I/O. If both types of ports exist in the same broadcast domain, LIF failover can result in moving data file I/O from a 10Gb port to a 1Gb port.

. Configure a failover group as user defined.

. Populate the failover group with ports on the SFO partner controller so that the LIFs follow the aggregates during an SFO. This approach avoids creating indirect traffic.

. Use failover ports with matching performance characteristics to the original LIF. For example, a LIF on a single physical 10Gb port should include a failover group with a single 10Gb port. A four-port LACP LIF should fail over to another four-port LACP LIF. These ports would be a subset of the ports defined in the broadcast domain.

. Set the failover policy to SFO partner only. Doing so makes sure that the LIF follows the aggregate during failover.

== Auto-Revert

Set the auto-revert parameter as desired. Most customers prefer to set this parameter to true to have the LIF revert to its home port. However, in some cases, customers have set this parameter to false so that they can investigate an unexpected failover before returning a LIF to its home port.

== LIF-to-Volume Ratio

A common misconception is that there must be a 1:1 relationship between volumes and NFS LIFs. Although this configuration is required for moving a volume anywhere in a cluster without creating additional interconnect traffic, it is categorically not a requirement. Intercluster traffic must be considered, but the mere presence of intercluster traffic does not create problems. Many of the published benchmarks created for ONTAP include predominantly indirect I/O.

For example, a database project containing a relatively small number of performance-critical databases that only requires a total of 40 volumes might warrant a 1:1 volume to LIF strategy. This arrangement would require 40 IP addresses. Any volume could then be moved anywhere in the cluster along with the associated LIF, and traffic would always be direct, minimizing every source of latency even at microsecond levels.

As a counterexample, a 1:1 relationship between customers and LIFs might make a large hosted environment easier to manage. Over time, you might need to migrate a volume to a different node, which would cause some indirect traffic. However, you shouldnâ€™t be able to detect the performance effect unless the network ports on the interconnect switch are saturating. If necessary, establish a new LIF on additional nodes and update the host at the next maintenance window to remove indirect traffic from the configuration.
